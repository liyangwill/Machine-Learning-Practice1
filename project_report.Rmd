---
title: "Practical Machine Learning Course Project Report"
author: "liyangwill"
date: "Febrary 06, 2017"
output:
  html_document:
    toc: yes
---

```{r prep, echo = F, message = F}
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)

set.seed(1234)
```

### Load data 
* Loading
```{r data, eval = F}
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
dim(training)
dim(testing)
```

* Partition traning set into subtraining (60%) and subtesting (40%)
```{r data, eval = F}
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
```

### Cleaning data
* Cleaning NearZeroVariance Variables  
```{r data, eval = F}
myDataNZV <- nearZeroVar(myTraining, saveMetrics=TRUE)
myNZVvars <- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
myTraining <- myTraining[!myNZVvars]
```

* Removing first column of Dataset, user name should not be indicative to result at all.
```{r data, eval = F}
myTraining <- myTraining[c(-1)]
```

* Plot the two features that have highest correlation with `classe` and color with `classe` to see if we can separate response based on these features.            
* Cleaning Variables with too many NAs. For Variables that have more than a 60% threshold of NAâ€™s they will be removed:
```{r data, eval = F}
training_prd <- myTraining 
for(i in 1:length(myTraining)) { #for every column in the training dataset
        if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { #if n?? NAs > 60% of total observations
        for(j in 1:length(training_prd)) {
            if( length( grep(names(myTraining[i]), names(training_prd)[j]) ) ==1)  { #if the columns are the same:
                training_prd <- training_prd[ , -j] #Remove that column
            }   
        } 
    }
}

dim(training_prd)
```

```{r data, eval = F}
myTraining <- training_prd
rm(training_prd)
```

* Apply cleaning processes to subtesting set, and remove `classes` in testing set
```{r data, eval = F}
step1 <- colnames(myTraining)
step2 <- colnames(myTraining[, -58]) #remove classes colume
myTesting <- myTesting[step1]
testing <- testing[step2]
```

## Using Ml algorithm

*Decision Tree
```{r data, eval = F}
DecTree <- rpart(classe ~ ., data=myTraining, method="class")
predictionsDT <- predict(DecTree, myTesting, type = "class")
confusionMatrix(predictionsDT, myTesting$classe)
```

* Radom forest
```{r data, eval = F}
RadForest <- randomForest(classe ~. , data=myTraining)
predictionsRF <- predict(RadForest, myTesting, type = "class")
confusionMatrix(predictionsRF, myTesting$classe)
```
The random forests algorithm generated a better model with higher accuracy.   

### Generating results to submit as answers for the Assignment:
```{r data, eval = F}
for (i in 1:length(testing) ) {
        for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}
#And to make sure Coertion really worked, simple smart ass technique:
testing <- rbind(myTraining[2, -58] , testing) #note row 2 does not mean anything, this will be removed right.. now:
testing <- testing[-1,]
predictions <- predict(RadForest, testing, type = "class")
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(predictions)
```
